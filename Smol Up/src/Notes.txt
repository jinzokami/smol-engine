Animation System:
#Have support for overriding animations per bone (so you can have a running animation and an aiming animation fire at the same time.)
Structs:

Animation - (AnimationData* animData, Orientation* startingOrientations, float timer = 0) #Maybe support blending between two animations (or animate the transition ourselves)
#Animation (Blending support) - (AnimationData* animData, Animation* previousAnim, float transDuration, Animation* blendingAnim = new Animation()) #blending anim is generated on anim creation if it exists it plays before anim
Animation Data - (Orientation start, Orientation end, Bone* bone, float startTime, float duration)
Orientation (Rotor) - (float scalar, Bivector bivector)
Bone - (Orientation localOrient, Orientation globalOrient, Bone* parent, Bone** children)

Methods:

Animation -
	play() - plays the animation, if paused resumes, if stopped restarts, if playing resets(? or just keeps going?). if a transition is playing plays that until it finishes no matter what.
	pause() - pauses the animation.
	stop() - stops the animation, if playing also resets, if paused then just resets
	reset() - resets the animation, if playing continues playing.
	seek(float time) - sets the animation timer, like reset, but more powerful.
	isLooping() - returns whether the animation loops
	setLooping(bool looping) - sets whether the animation loops
	onFinish(function action) - sets the action to take when the animation finishes playing (play another animation, make someone else play an animation, bring up a menu, you know, whatever)

	Specifics: Starting from some root bone, calculate the Orientation by blending between the start and end orientation depending on an alpha value dictated by clamp((current time - start time)/duration, 0, 1)
		whenever the alpha value is 1 fire off the finish action and reset timer to 0 (or subtract duration from timer to get the spot in the animation it needs to be in)
		Animation Data will be sorted from earliest start to latest start, the orientation of the bone will be decided by the Animation Data with the latest start time BEFORE the timer value
		every bone child will have its parent bones orientation added on to it, this will be the final orientation for the bone. ALL BONES MUST BE ACCESSIBLE FROM AT LEAST A ROOT BONE.

Animation Data -
	Specifics: no associated methods since these are meant to be interpreted by the animation. These will be output to binary files by an animation editor program.

Bone - 
	Specifics: these will have no associated methods since these are meant to be interpreted by the renderer. Animated Models will attach (reference) Bones to each sub-Model, each sub-Model's Bone will then be attached to Animation Data


Shaders:
Standard uniforms (supported by all shaders by default) will include:
 mouse position
 a global timer (runs from program start)
 a local timer (runs from shader init)
 light positions in the scene
 light colors in the scene
 direction of "sunlight"
 color of "sunlight"
 color of ambient light
 intensity of ambient light

Shader manager
 only one instance of a shader needs to exist on the gpu at any point.
 a shader manager will keep track of compiled shaders and avoid recompilation of already extant shaders.
 underlying structure is an unordered_map, the key is the file name of the shader files, without directory or file extension: for example, "basic" instead of "res/basic.vert" and "res/basic.frag"
 shader files will then have to share filenames and the only distinguishing feature between them would be file extensions: "vert", "frag", "geom", "tess", "comp"

VBOs
 until now, I have been using multiple VBOs to define each object for rendering. 
 This requires (as far as I can tell) multiple VAOs. 
 switching VAOs is costly in the long run and complex.
 a much better solution is putting together everything in one VBO. 
 obviously, this means all objects in that VBO are rendered exactly the same way.
 Mesh objects will no longer store their vertices, that functionality will be transferred to a new ProcMesh object for the sake of simplicity.
 when Mesh objects are created, their vertices are sent to the corresponding buffer on the GPU, we'll have to learn how to manage that memory.
 Meshes will then store their vbo id, their start position in said vbo, and their vertex count. that's it.

 the plan:
  preallocate some space on the gpu (but still support growing the preallocated space)
  when a mesh is created, send the vertices to the preallocated vbo and figure out the start position of these.
  idea: keep a list of the start and end offsets for each mesh in the vbo, when the vbo is reallocated, take that opportunity to defrag memory, otherwise do so when low on allocated memory.
   when a mesh is destroyed, delete it's corresponding entry in the list, when reorganising memory, write over parts not on the list.
  on draw, in glDrawArrays() pass in the start position and vertex count for the current mesh. done.
  
  considerations:
   shaders need to be rebound as well, so it's best to sort by shader, to have all meshes with the same shader render one after the other.
   textures also need to be rebound, so it's best to sort each shader by texture, to minimize texture rebinds.
   this means textures, shaders, and vbos/vaos all need ids. luckily, all of these are assigned ids by opengl, so just sort by those.
   sorting order is as follows, vbo, shader, texture1, texture2, texture3, etc.
   this should minimize rebinds when a few assets are reused many times.

VAOs
 there will be a VAO for each kind of mesh, terrain, particle effects, decorations, objects. done.

Levels
 levels will be made of Cells. 
 Cells are 3D units made of Subcells that are either solid or not. 
 solid Subcells can have a height, they always start from the bottom of the cell.
 visually, Subcells will be constructed from prefab parts made in either some custom tool or Blender.
 there will be default visuals for Subcells as well.
 the default visuals should represent the actual geometry that calculations use.
 default visuals will be:
 a flat cell with top=height.
 a slanted cell in either the X +/- or Z +/- direction with the slant going from the lowest of either height or adjacent height, to the highest of the two.
 walls will stretch from the top of the cell down to the bottom of the cell. this could and should be optimized.
 pits with no Cells below are kill planes.
 pits with Cells below deal damage proportional to the number of empty cells between player and floor.
 if the player lands on a "soft" cell then no damage is taken.
 player is 3.5 units tall.
 player can walk over height differentials of <= 1 unit.
 slants of difference > 1 should be prefab, player can't walk them, I don't care about them.

Mouse:
 we may need to click things on screen with the mouse, so it's useful to know where the mouse is pointing.
 for this, we need to cast a ray from the camera, through the spot on the screen the mouse was pointing at, to the scene.
 to this end, we must calculate the distance from camera to screen.
 the only information we have available to us, however, is the field of view, and the dimensions of the screen. this is enough.
 Spoiler: the answer is cos(fov/2) / sin(fov/2) or 1/tan(fov2), but tan is scary, there be dragons.
 explanation: 
  the field of view used by the perspective matrix constructed by glm is the y-axis fov.
  that means the angle of the isosceles triangle formed by the camera point and the "screen height" is whatever value the fov is. (given that the camera is centered on the screen plane, that is, on x = 0, y = 0)
  since we have no idea how far away the camera is from the screen plane, we only have one angle on this isosceles triangle. that is not enough information.
  so we split the triangle into two right triangles. the screen side is split in half, leaving the other two sides untouched. 
  we now have two angles: the right angle and the angle we already had, the field of view, just this time, it's also been split in half.
  we can use the two sides we have to solve for the remaining side, which comes out to 90 - fov/2 or PI/2 - fov/2 depending on whether you're working with radians or degrees (no one likes gradients)
  from here we can use the sine rule to find the length of the side we care about, the side that goes from the middle of the screen to the camera.
  the sine rule says that the ratio between a side and its opposing angle is the same for all sides in a triangle. 
  so we have to solve for X in the following equation(A is the length of the side we already know): A/sin(fov/2) = X/sin(90 - fov/2)
  having solved for X we get: X = sin(90 - fov/2) / sin(fov/2)
  sin(90 - fov/2) = cos(fov/2) which means X = cos(fov/2) / sin(fov/2)
  this is because sin(A-B) = sin(A) * cos(B) - cos(A) * sin(B) which in this case evaluates to: sin(90) * cos(fov/2) - cos(90) * sin(fov/2)
  sin(90) = 1, cos(90) = 0 which cancels the sin(fov/2), leaving us with cos(fov/2)
 the result is we now have the screen displacement relative to the camera, so we just use the position of the mouse on screen as the x and y coordinates and the screen displacement as the z, then we transform this "ray" to place it in the correct spot and orientation in "world space"

"Spaces":
 3D renderers typically have multiple coordinate systems that objects live in, here are the important ones (though, depending on your code not necessarily the only ones)
 Object (Model, Local) Space:
  object space is where the vertices on a 3D model land relative to some point on the original model (usually the origin of the coordinate system used by the 3D modelling program used to create the model)
  when an animation system moves vertices around, they do it in model space.
  the model matrix brings object space to world space,

 World Space:
  world space is where models go when they are placed in the... world.
  this is typically the coordinate system used by a physics engine, so this is the coordinate system we care about the most for non-rendering related things.
  the view matrix brings world space to camera space.

 Camera (Eye, View) Space:
  camera space is the world from the perspective of the camera, every movement the camera makes, the opposite happens to everything else. it is the reference point here.
  this has one other space directly associated with it, the projection space. 
  projection space is what decides where each vertex goes as you get further from the camera. perspective projection moves points closer to the origin with distance, orthographic projections do nothing, and therefore have no sense of depth.
  in most cases the projection is a perspective projection, though there are as many projections as there are uses for a 3D renderer.
  the projection brings objects into the clip space

 Clip Space:
  this is where the "game" stuff becomes pure "renderer" stuff. in this space everything is oriented such that everything on screen is within some kind of box.
  everything outside of this box isn't rendered. and if you can say with certainty that an object is not within this box, you can cull it entirely.

 NDC:
  Normalized Device Coordinates, basically the same thing as clip space but highly dependent on the render device. is the penultimate before the fragment, or pixel, shader.

 Screen Space:
  The final coordinate system, this one brings everything into actual screen coordinates, these are the "gl_FragCoord" variables in glsl, minus the depth, which is used for special effects, but real screens don't have depth (unless it's a 3DS, and that's debatable).

to get the coordinate of a mouse click in world space we need to go from screen space (pixel coordinates) to NDC to Clip Space to View Space to World Space.
Screen to NDC is simple enough: mouse.x/screen.width, mouse.y/screen.height
NDC to Clip is: mouse_ndc.x * screen.width/screen.height (the aspect ratio), mouse_ndc.y * screen.height/screen.width (inverse aspect ratio)
Clip to View: 